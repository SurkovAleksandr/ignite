Пушенко Кирилл Валерьевич  
Саша, если кратко проблема состоит в следующем: стартуют две ноды идет вставка в кеш, потом чтение можно сказать последовательно. При этом запись проходит успешно, но чтение периодически нет

+ Плеханов Алексей - ребят можете Леше тоже прав насыпать

Андрей, а по гатлину мы отправили put по rest мы дожидаемся какого-то ответа?

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
15:00
Да, ожидаем ответ 200 везде. Шаги гатлина последовательно идут

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
15:28
Ок

Пока права не получил, такой вопрос: FULL_ASYNC режим кэша явно не задавали?

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
17:42
Попробуйте FULL_SYNC режим кэша поставить

Фомин Андрей Николаевич  
17:42
Попробуйте FULL_SYNC режим кэша поставить
Спасибо, помогло на стабильной топологии. Теперь get после put всегда успешно проходит.

Попробовал менять топологию - начали появляться различные проблемы.
Немного подебажил ⁣ ⁣- добавил параметр -DIGNITE_NO_SHUTDOWN_HOOK=true и 
еще теперь останавливаю ноду через Ignition.stop(ignite.name(), false), т.к. метод ignite.close() отменяет текущие таски.
Но сейчас по прежнему есть проблема - не находятся значения в кеше при уменьшении количества нод.

Пробовал делать cacheCfg.setAtomicityMode(CacheAtomicityMode.TRANSACTIONAL); - не влияет

Пушенко Кирилл Валерьевич  
@kvpushenko.sbt  
07:46
Сурово) какой кейс увеличения - уменьшения? Сколько узлов было стало

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
07:53
При увеличении всё ошибок нет. Ошибки при переходе от трёх нод к двум

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
08:30
Ошибка какая? Также нет значения, хотя вставка успешно прошла?

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
08:50
Да, нет значения

Пушенко Кирилл Валерьевич  
@kvpushenko.sbt  
10:13
Леха, может все таки отключить чтение из бекапов?

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
10:14
Я думаю тут не в них проблема, но можно конечно попробовать

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
10:16
Еще раз давай сценарий проговорим, данных нет, запускаются три ноды, выполняется put, убивается нода, выполняется get, так?

Или какие-то еще действия этому предшествуют, типа входы выходы других нод, наполнение какими-то данными и т.д.?

Если до этого например было 4 ноды, в гриде есть какие-то данные, убили одну ноду, затем через некоторое время еще одну, то в этом случае может быть потеря данных из-за того что между убийством первой и второй нодой может не успеть пройти ребаланс

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
10:20
1) Первоначально создан кластер из 2-х нод
2) Запускаю сценарий ConstantLoadSimulation - там генерируется нагрузка от 500 пользователей в секунду, которые делают по 3 запроса
далее под нагрузкой
3) увеличиваю количество нод до 3-х - все норм
4) уменьшаю количество нод до 2-х - вижу ошибки

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
10:36
Странно очень

Есть еще подозрение вот здесь:

        IgniteFuture<Void> createFuture = map.putAsync(uuid, code);
        return Mono.create(monoSink ->
                createFuture.listen(future ->
                        monoSink.success(uuid.toString())
                )
        );

Не проверяешь результат фьючи

Т.е. она закомплитилась, но могла закомплитится с ошибкой

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
10:37
Кстати, да. Сейчас добавлю

Отключил чтение из бекапов - пропали ошибки.
Но вообще не хотелось бы их отключать.

Ну и не понятно, как это влияет

Пушенко Кирилл Валерьевич  
@kvpushenko.sbt  
10:38
зависит от профиля, сколько процентовка запись/чтение

видимо не успевает отребалансить, но это похоже на багу

>Плеханов Алексей Сергеевич  
	Он вообще не должен принимать во внимание этот бэкап, пока ребаланс не закончится

Пушенко Кирилл Валерьевич  
@kvpushenko.sbt  
10:40
я в это не верю) есть тест на такую проверку?

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
10:40
В общем то что помогло отключение чтений из бэкапов - это еще более странно

Пушенко Кирилл Валерьевич  
@kvpushenko.sbt  
10:40
я в это не верю) есть тест на такую проверку?
> CacheGetReadFromBackupFailoverTest


Фомин Андрей Николаевич  
@fomin.a.nikolaev  
10:58
Добавил проверку результата фьючи на put - там ошибок не вижу
Вернул чтение из бекапов - вновь вижу ошибки

Попробовал еще добавить задержку 5 секунд между записью и чтением - резульат не меняется.

В логе вижу вот такие ошибки
Dec 24, 2019 8:10:48 AM org.apache.ignite.logger.java.JavaLogger error
SEVERE: Failed to send message to remote node [node=TcpDiscoveryNode [id=7de97a78-f1e8-47a7-a692-5ccf3f12ba9e, addrs=[0:0:0:0:0:0:0:1%lo, 10.128.1.166, 127.0.0.1], sockAddrs=[/0:0:0:0:0:0:0:1%lo:47500, /127.0.0.1:47500, /10.128.1.166:47500], discPort=47500, order=5, intOrder=4, lastExchangeTime=1577175033623, loc=false, ver=2.7.6#20190911-sha1:21f7ca41, isClient=false], msg=GridIoMessage [plc=2, topic=TOPIC_CACHE, topicOrd=8, ordered=false, timeout=0, skipOnTimeout=false, msg=GridNearSingleGetRequest [futId=1577173893406, key=UserKeyCacheObjectImpl [part=896, val=04e5b646-4220-4997-9fb5-883e80dff1c0, hasValBytes=true], flags=1, topVer=AffinityTopologyVersion [topVer=5, minorTopVer=1], subjId=fbd6fd6d-1015-4407-8962-26bad5cdbd76, taskNameHash=0, createTtl=600000, accessTtl=-1, mvccSnapshot=null]]]
class org.apache.ignite.internal.cluster.ClusterTopologyCheckedException: Failed to send message (node left topology): TcpDiscoveryNode [id=7de97a78-f1e8-47a7-a692-5ccf3f12ba9e, addrs=[0:0:0:0:0:0:0:1%lo, 10.128.1.166, 127.0.0.1], sockAddrs=[/0:0:0:0:0:0:0:1%lo:47500, /127.0.0.1:47500, /10.128.1.166:47500], discPort=47500, order=5, intOrder=4, lastExchangeTime=1577175033623, loc=false, ver=2.7.6#20190911-sha1:21f7ca41, isClient=false]

Пушенко Кирилл Валерьевич  
@kvpushenko.sbt  
11:23
а можешь весь лог переслать, нужно понять pme там закончилось

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
11:25
>Тут неполные логи и нет логов с остановленной ноды. Чтоб полные получить нужно настроить логирование наружу
	А что происходит в кластере из трех нод, когда запрашиваемый мастер-партишен находится на первой ноде, 
	а его бекап - на второй. Если мастер упал, а запрос пришел на третью ноду. Какое поведение в этом случае?

Пушенко Кирилл Валерьевич  
@kvpushenko.sbt  
11:37
Андрей, а можешь еще проверить утилитой после того как стало три ноды

control.sh -baseline

Фомин Андрей Николаевич  
	Я использую embedded игнайт, который на центральном мавене опубликован

Пушенко Кирилл Валерьевич  
	https://ignite.apache.org/download.cgi скачай пожалуйста, у меня есть подозрение, что третий узел пустой и не в топологии

Плеханов Алексей Сергеевич  
>::А что происходит в кластере из трех нод, когда запрашиваемый мастер-партишен находится на первой ноде, а его бекап - на второй. Если мастер упал, а запрос пришел на третью ноду. Какое поведение в этом случае?
	Третья нода идет на первую, если еще не знает что она упала, получает отказ, делает ретрай несколько раз. После того как информация об ушедшей ноде распространяется в кластере, пвторая нода становится праймари, начинается ребаланс со второй ноды на третью, чтобы осталось заданное количество копий в кластере. Пока ребаланс не закончен запросы идут на вторую.

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
11:41
В логе первых двух узлов есть сообщения, когда присоединяется третий. Могу заново стартовать три узла и прислать логи запуска.

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
11:41
Посмотри еще в логах сообщение Detected lost partitions

Пушенко Кирилл Валерьевич  
@kvpushenko.sbt  
11:37
control.sh -baseline
Здесь же in-memory, бэйзлайна для in-memory в 2.7.6 не было

Пушенко Кирилл Валерьевич  
@kvpushenko.sbt  
11:49
точно

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
12:50

(изменено)
Вот логи с трех нод. Третья нода была остановлена

По логу видно, что исключение происходит вот на этой строчке
IgniteFuture<Integer> getFuture = map.getAsync(keyUuid);

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
13:45
А если read from backup отключаешь эксепшнов в логе тоже нет?

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
13:45
Похоже, в этом исключении ничего страшного - вполне ожидаемая ситуация, когда нода покинула кластер, зачем-то логируется стек-трейс там.

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
13:45
А если read from backup отключаешь эксепшнов в логе тоже нет?
Сейчас проверю

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
13:47
Фомин Андрей Николаевич  
@fomin.a.nikolaev  
13:45
Похоже, в этом исключении ничего страшного - вполне ожидаемая ситуация, когда нода покинула кластер, зачем-то логируется стек-трейс там.
Ну там тоже странность есть, нода уже определила, что версия топологии 4, а эксепшены еще долго валятся про версию топологии 3

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
13:53

Вот логи с оключенным чтением бекапов

Ошибок в тесте не вижу при этом

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
13:58
А у тебя ассерты выключены?

Есть подозрение вот на этот тикет https://issues.apache.org/jira/browse/IGNITE-10352, он только в 2.8 исправлен, но там поведение немного другое, по ассертам валится

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
14:01
Выключены, да

А где можно взять 2.8?

https://mvnrepository.com/artifact/org.apache.ignite/ignite-core зедсь нет

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
14:02
Он еще в процессе релиза

Через месяц наверное только будет

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
14:04
А это объясняет почему запрет на чтение из бекапов помогает?

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
14:11
Фомин Андрей Николаевич  
@fomin.a.nikolaev  
14:04
А это объясняет почему запрет на чтение из бекапов помогает?
Чтобы точно сказать нужно вникать в фикс по этому тикету, но гипотеза почему такое может случиться есть (нода уходит, текущая нода становится backup нодой для партиции, состояние у нее MOVING, т.е. из нее пока читать нельзя, но если верить утверждению в тикете то иногда такое чтение может проходить и в этом случае получим неконсистентный результат, т.е. можем прочитать из нее данные, но данные на нее еще не приехали)

Если read from backup выключить - чтение всегда пойдет на primary ноду, нода в MOVING состоянии primary быть не может, поэтому в этом случае мы всегда будем брать результат с ноды на которой все данные есть

Попробуй ассерты включить

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
14:14
Сейчас уже убегаю. Чуть позже попробую

Пушенко Кирилл Валерьевич  
@kvpushenko.sbt  
14:44
Мы можем собрать dev сборку только с этим фиксом

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
18:21
Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
14:13
Попробуй ассерты включить
Включил - ничего не изменилось

Пушенко Кирилл Валерьевич  
@kvpushenko.sbt  
14:44
Мы можем собрать dev сборку только с этим фиксом
Собрать то и я могу. Не пойму, а где в https://issues.apache.org/jira/browse/IGNITE-10352 ссылка на сам влитый ПР. Там ссылки на разные ПР-ы котрорые не были влиты.

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
18:31
5519 влит

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
18:34
И с 30-го ноября прошлого года не было релизов из ветки master?

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
18:36
Нет, 2.7 был в ноябре, потом еще два релиза на основе 2.7 было, 2.8 в процессе

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
18:42
Нда, смотрю на этот ПР - в gird gain особо не заморачиваются с чистотой коммитов )

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
18:46
Почему? Конкретно в этом вроде ничего критичного нет, немного кодстайл в тесте не соблюден

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
18:53
Был бы один коммит, в котором только одна проблема решается (без всяких там фиксов запятых в соседних строчках и исправлений на исправление) - легче было бы черри-пикнуть. https://chris.beams.io/posts/git-commit/ тут хорошо всё сказано.

Пушенко Кирилл Валерьевич  
@kvpushenko.sbt  
14:44
Мы можем собрать dev сборку только с этим фиксом
А как собрались делать. Наложить черри-пикнуть коммиты из этог ПР поверх текущей стабильной версии?

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
18:58
Фомин Андрей Николаевич  
@fomin.a.nikolaev  
18:53
Был бы один коммит, в котором только одна проблема решается (без всяких там фиксов запятых в соседних строчках и исправлений на исправление) - легче было бы черри-пикнуть. https://chris.beams.io/posts/git-commit/ тут хорошо всё сказано.
Так тут и есть один коммит на одну проблему :)

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
18:56
А как собрались делать. Наложить черри-пикнуть коммиты из этог ПР поверх текущей стабильной версии?
Да

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
19:07
Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
18:58
Так тут и есть один коммит на одну проблему :)
Нормальный коммит с нормальным сообщением выглядит вот так https://github.com/torvalds/linux/commit/23f6b02405343103791c6a9533d73716cdf0c672
А вот тут что-то зачем-то поправил https://github.com/apache/ignite/pull/5519/commits/65fe6ff96055e7fd1b6f3a4fb8c3a8e4e04eb391
Просто у меня больная тема со всеми этими коммитами )

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
19:08
Так нужно не сами коммиты в ПР смотреть, а итоговый коммит

https://github.com/apache/ignite/commit/18aaee039908821d9d962a7bf3659578164e375d

В мастер он идет

Внутри ПР разработчик сам решает как и сколько коммитов делать

Откатывать и накатывать заново и какие сообщения к коммиту

В итоговом коммите все ок, заголовок в порядке, он атомарен

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
19:11
Т.е. там сама система объединяет коммиты перед мержем?

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
19:12
Если через гитхаб вливать - да, он сам умеет. Если вручную, то тот кто мержит соответственно делает git pull --squash

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
19:12
Теперь понятно, почему ПР отображается как закрытый, а не влитый

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
19:17
Часть руками мержится, они переходят в "closed" когда в мастере появляется коммит с указанием номера PR, часть мержится через гитхаб, в этом случае ПР будет в состоянии "merged". Чтобы через гитхаб вливать в апачевские проекты нужно двухфакторную аутентификацию настраивать, не всегда удобно

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
20:21
Собрал ветку 2.8 - с ней нормально работает.

Думаю, нет смысла черри-пикать исправление. Там конфликты. И даже если их решить, не факт, что не появится других проблем. Думаю, надо либо делать повторные запросы со стороны клиента (как воркэраунд), либо уже идти со сборкой из ветки 2.8

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
21:02
2.8 еще не стабильна, рискованно с ней идти

Лучше read from backup отключить до 2.8

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
21:09
Да, можно так

Ну, в пром мы все равно пока не идем. В пром еще через месяца три в лучшем случае

Пушенко Кирилл Валерьевич  
@kvpushenko.sbt  
21:35
Мы успеем выпустить 2.8) там действительно много изменений. Повтор на клиенте лучше реализовать в любом случае, на всякий )

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
21:44
Повтор на клиенте я не понимаю зачем, если при попытке получить значение происходит ошибка, то игнайт сам делает повтор (если не задан модификатор кэша withNoRetries), если не происходит ошибки а возвращается пустое значение, то эта ситуация не отличима от просто отсутствия значения. И повтор в тот же момент (если будет выполнен на том же сервере) приведет к тому же результату.

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
08:36
Я думал делать повтор с задержкой

Ну лучше действительно так не делать. Не надо усложнять клиент. И не понятно что означает пустое значение, то ли его никогда не было, то ли оно уже ушло из кеша, то ли еще не проша миграция партиции.

Вопрос еще есть по подовду двух дата-центров. Данные нужно реплицировать между ДЦ на случай падения одного из них. Каким образом это лучше сделать?

И еще не понятно, как ведет себя ignite в случае сегментации кластера. Нужно ли что-то дополнительно настраивать.

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
12:58
Фомин Андрей Николаевич  
@fomin.a.nikolaev  
12:18
Вопрос еще есть по подовду двух дата-центров. Данные нужно реплицировать между ДЦ на случай падения одного из них. Каким образом это лучше сделать?
https://ignite.apache.org/releases/latest/javadoc/org/apache/ignite/cache/affinity/rendezvous/ClusterNodeAttributeAffinityBackupFilter.html

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
12:43
И еще не понятно, как ведет себя ignite в случае сегментации кластера. Нужно ли что-то дополнительно настраивать.
Два варианта, 1 - написать свой Topology Validator (из коробки готового нет), 2 - Использовать zookeeper discovery SPI, там есть какое-то решение по поводу split brain, но не уверен что можно использовать его из коробки (не смотрел просто)

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
13:03
А если вообще ничего не настраивать, то что произойдет после объединения сегментов?

(изменено)
Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
12:58
https://ignite.apache.org/releases/latest/javadoc/org/apache/ignite/cache/affinity/rendezvous/ClusterNodeAttributeAffinityBackupFilter.html
Как понимаю, тут речь идет о едином кластере, который разделен между ДЦ. А есть варианты с репликацией между независимыми кластерами? Между ДЦ, например, не получится сделать node discovery.

Как вариант, думал от том, чтобы само приложение записывало значения на противоположенный ДЦ

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
13:29
Фомин Андрей Николаевич  
@fomin.a.nikolaev  
13:03
А если вообще ничего не настраивать, то что произойдет после объединения сегментов?
Будут два независимых кластера

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
13:14
Как понимаю, тут речь идет о едином кластере, который разделен между ДЦ. А есть варианты с репликацией между независимыми кластерами? Между ДЦ, например, не получится сделать node discovery.
В игнайте нет. Есть в GG платном пара вариантов, синхронный транзакционно неконсистентный и асинхронный транзакционно консистентный

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
13:22
Как вариант, думал от том, чтобы само приложение записывало значения на противоположенный ДЦ
В принципе подобный вариант сейчас в ППРБ используется (прикладной журнал)

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
13:43
Только совсем нетривиально в реализации выглядит вариант с тем, чтобы сделать полноценную репликацию между ДЦ. Та же распределенная БД, только на уровне кластеров уже. Видимо, первоначально надо по-простому делать - пусть часть данных теряется при переключении на другой ДЦ.

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
13:53  (изменено)
Для СМС кодов вроде все просто, put в два кластера в разных ДЦ, get из любого

Хотя с сегментацией конечно нужно еще что-то придумывать

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
14:10
Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
13:00
Два варианта, 1 - написать свой Topology Validator (из коробки готового нет), 2 - Использовать zookeeper discovery SPI, там есть какое-то решение по поводу split brain, но не уверен что можно использовать его из коробки (не смотрел просто)
На SegmentationResolver смотрел. Потенциально из кубернетеса можно брать ожидаемое количество нод в кластере (N). И самоубиваться, если в кластере нод меньше чем N/2, например.

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
14:27
SegmentationResolver - не используется в игнайте, это куски кода, которые остались с тех пор как игнайт был выделен из кода ГГ, хотя можно конечно написать свой GridSegmentationProcessor чтобы он дергал SegmentationResolver, это десяток строчек кода. SegmentationResolver не дает строгих гарантий консистентности (т.к. дергается по таймауту), хотя в вашем случае возможно того что он дает будет достаточно. Ну и опять же он применим только в случае растянутого на два ДЦ кластера

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
14:40
А почему два ДЦ? Чтоб данные не терять при сементации?

Не, не понял про кластер на два ДЦ

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
14:42
Ну ты говорил что нужно не кластер на два ДЦ, а в каждом ДЦ по кластеру. SegmentationResolver в этом случае мало чем поможет

Пушенко Кирилл Валерьевич  
@kvpushenko.sbt  
14:43
Да есть два пути один большой кластер на два ДЦ с резолвером. И два ДЦ с репликацией

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
14:46
Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
14:42
Ну ты говорил что нужно не кластер на два ДЦ, а в каждом ДЦ по кластеру. SegmentationResolver в этом случае мало чем поможет
Я про то, чтоб в рамках одного ДЦ не получить split brain. Напрмер, потоки подвиснут или слишком много сетевых соединений будет.

@kvpushenko.sbt, а можем запланировать ВКС с нашими архитекторами? Обсудим голосом прототип, который получился и нужно ли нам еще что-то, чтобы уже начать пилить реальное приложение. И ты еще говорил, что можно поговорить с вашим архитектором.

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
15:03
Фомин Андрей Николаевич  
@fomin.a.nikolaev  
14:46
Я про то, чтоб в рамках одного ДЦ не получить split brain. Напрмер, потоки подвиснут или слишком много сетевых соединений будет.
Обычно такие вещи не приводят к сплит брэйну, если ноде плохо и она перестает адекватно отвечать то либо она сама гасится либо ей говорят что "кластер тебя выкинул" и тогда она завершает работу. Сплит брэйн в принципе в одном ДЦ может происходить при обрыве связи между нодой (или набором нод) и другими нодами, но еще раз повторю SegmentationResolver не лучший способ его решить, во первых его нельзя использовать в игнайте (без создания плагина), во вторых между сегментацией и временем когда он сработает есть период, когда клиент может данные теоретически поменять, потом эти изменения потеряются

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
15:04
Т.е. в рамках одного ДЦ не надо заморачиваться с сегментацией?

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
15:18
Ну хз, у вас еще кубер свои коррективы вносит, в банке никто с ним игнайт не использовал, там вроде соединения могут нестабильные быть, возможно риск сегментации с ним выше

Пушенко Кирилл Валерьевич  
@kvpushenko.sbt  
15:27
Пушенко Кирилл Валерьевич исключил(-а) Осипов Виталий Александрович

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
08:27
Попробоавл сейчас потестить, что будет при переполнении кеша - по умолчанию просто убивает jvm. Настроил DataPageEvictionMode.RANDOM_LRU - и тут возможны варианты. С нагрузкой 500 запросов в секунду - все встает колом. С нагрузкой 100 запросов в секунду - работает.

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
10:43
Убивает JVM с причиной IgniteOutOfMemoryException и сообщением "Critical system error detected. Will be handled accordingly to configured handler"? Если да, то это ожидаемо. Можно настроить другое поведение, но по умолчанию убивать JVM это меньшее из зол.

Про встает колом как это проявляется? Сам эвикт страницы конечно не моментальная операция, но он должен случаться не на каждый пут, если у вас UUID + Integer, размер записи маленький, на страницу таких записей 50-100 должно поместится, т.е. эвикт один раз на 50-100 путов теоретически должен случаться.

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
11:47
Полностью загружает процессор и перестает овечать на http-запросы

При этом нагрузку убрал, но продолжает висеть в подобном состоянии


Ни разу не дожидался, чтобы "отвисло"

Алесей, еще есть вопрос по метрикам. Добавил вывод метрик. Например, вот так выглядят
Memory Region Name: sysMemPlc
Allocation Rate: 0.0
Pages Fill Factor: 0.0
Total Allocated Size: 824000
Physical Memory Size: 824000
Off-heap Size: 41943040
Off-heap Used Size: 0

Memory Region Name: default
Allocation Rate: 35.9
Pages Fill Factor: 0.9993018
Total Allocated Size: 8874480
Physical Memory Size: 8874480
Off-heap Size: 10485760
Off-heap Used Size: 8874480

Memory Region Name: TxLog
Allocation Rate: 0.0
Pages Fill Factor: 0.0
Total Allocated Size: 0
Physical Memory Size: 0
Off-heap Size: 41943040
Off-heap Used Size: 0

Для кеша установил, что значения нужно хранить 10 минут. Через 10 минут не вижу, чтобы какие-либо значения тут уменьшались.

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
12:00
Так они и не уменьшатся, разве что pages fill factor может поменяться и allocation rate

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
12:01
А как увидеть, что записи удаляются из кеша?

Пушенко Кирилл Валерьевич  
@kvpushenko.sbt  
12:01
по первой проблеме это случайно не этот тикет: https://issues.apache.org/jira/browse/IGNITE-11438.

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
12:03
Кстати, сейчас добавлю в тест еще запрос через 10 минут - проверю есть ли записи через это время. У меня вопрос то первоначальный именно как увидеть, сколько памяти фактически используем, чтобы понять, нужно ли добавлять ноды в кластер.

Или наоборот убрать лишние ноды

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
12:08
Фомин Андрей Николаевич  
@fomin.a.nikolaev  
12:01
А как увидеть, что записи удаляются из кеша?
По метрикам в 2.7 этого не понять, можно определить только используя метрики из internal API, что нежелательно. В 2.8 будет метрика которая показывает сколько реально используемых страниц

Сам факт эвикта (для теста) можно проверить например вставив записи и проитерировавшись после эвикта по записям кэша

Либо листенер на эвикт поставить

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
12:10
Да, добавил в тест такую проверку - работает

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
12:08
По метрикам в 2.7 этого не понять, можно определить только используя метрики из internal API, что нежелательно. В 2.8 будет метрика которая показывает сколько реально используемых страниц
А как через internal api получить?

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
12:13
long emptyPages = ((AbstractFreeList)((IgniteEx)ignite).context().cache().context().database().freeList(DATA_REGION)).emptyDataPages();

DataRegionMetrics metrics = ignite.dataRegionMetrics(DATA_REGION);

long usedSize = ((long)(metrics.getTotalAllocatedPages() * metrics.getPagesFillFactor()) - emptyPages) * metrics.getPageSize();

Но повторюсь, в продашн такое нельзя, совместимость internal API не гарантируется, может сломатся при любом обновлении

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
12:32
Спасибо, работает такой вариант

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
13:35
Фомин Андрей Николаевич  
@fomin.a.nikolaev  
11:47
Полностью загружает процессор и перестает овечать на http-запросы
По зависающему эвикту страниц, можешь логи посмотреть? Есть там что-нибудь типа "Too many attempts to choose data page" или "Too many failed attempts to evict page"?

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
19:04
В логе никаких новых сообщений после старта

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
21:25
Имелся ввиду момент зависания, на тот момент когда тред дамп формировал, логов не сохранилось?

Фомин Андрей Николаевич  
@fomin.a.nikolaev  
08:08
При зависанни нет никаких логов. Если надо, могу закоммитить конфигурацию, на которой воспроизводится проблема. Но для нас, думаю, эта проблема не критична.

Плеханов Алексей Сергеевич  
@asplekhanov.sbt  
09:12
Ок, не нужно. Когда-нибудь попробую репродюсер сам написать